{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe5ed1d1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0407469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0bdbff",
   "metadata": {},
   "source": [
    "# Data from TwelveData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f8435",
   "metadata": {},
   "source": [
    "First you have to create an account [here](https://twelvedata.com) and the generate a key [here](https://twelvedata.com/account/api-keys). Place the key in a file named .env and make sure there exists this field:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186bda1",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Do **not** share this key or commit the `.env` file to version control. It's already excluded in `.gitignore`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b50af",
   "metadata": {},
   "source": [
    "> TD_API_KEY=<yllaertubyrtecinreadbackwards>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d24e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twelvedata import TDClient\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed5aae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"TD_API_KEY\")\n",
    "db_username = os.getenv(\"db_username\")\n",
    "db_password = os.getenv(\"db_password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dcae8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          open     high        low      close  volume\n",
      "datetime                                                             \n",
      "2025-05-09 15:59:00  198.45500  198.550  198.39999  198.53999  787957\n",
      "2025-05-09 15:58:00  198.46880  198.500  198.38000  198.45010  199966\n",
      "2025-05-09 15:57:00  198.38000  198.510  198.37869  198.47000  125862\n",
      "2025-05-09 15:56:00  198.49500  198.495  198.34010  198.37981  111684\n",
      "2025-05-09 15:55:00  198.75999  198.765  198.42999  198.50999  119482\n"
     ]
    }
   ],
   "source": [
    "# Initialize client\n",
    "td = TDClient(apikey=api_key)\n",
    "\n",
    "# Fetch data\n",
    "ts = td.time_series(\n",
    "    symbol=\"AAPL\",\n",
    "    interval=\"1min\",\n",
    "    outputsize=100,\n",
    "    timezone=\"America/New_York\",\n",
    ")\n",
    "\n",
    "df = ts.as_pandas()\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e096e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index().to_csv(\"data/raw/AAPL_1min.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb333a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 open       high        low      close    volume\n",
      "datetime                                                        \n",
      "2024-12-30  587.89001  591.73999  584.40997  588.21997  56578800\n",
      "2024-12-27  597.53998  597.78003  590.76001  595.01001  64969300\n",
      "2024-12-26  599.50000  602.47998  598.08002  601.34003  41219100\n",
      "2024-12-24  596.06000  601.34003  595.46997  601.29999  33160100\n",
      "2024-12-23  590.89001  595.29999  587.65997  594.69000  57635800\n"
     ]
    }
   ],
   "source": [
    "# Fetch daily time series data for SPY from 2024-01-01 to 2024-12-31\n",
    "ts = td.time_series(\n",
    "    symbol=\"SPY\",\n",
    "    interval=\"1day\",\n",
    "    start_date=\"2024-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    timezone=\"America/New_York\"\n",
    ")\n",
    "\n",
    "# Convert the time series data to a pandas DataFrame\n",
    "df = ts.as_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save the DataFrame to a CSV file\n",
    "df.to_csv(\"data/raw/SPY_2024_daily.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176fdffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 open       high        low      close      volume\n",
      "datetime                                                          \n",
      "2024-12-01  602.96997  609.07001  580.90997  586.08002  1059516700\n",
      "2024-11-01  571.32001  603.34998  567.89001  602.54999   901843000\n",
      "2024-10-01  573.40002  586.12000  565.27002  568.64001   976068800\n",
      "2024-09-01  560.46997  574.71002  539.44000  573.76001  1045061400\n",
      "2024-08-01  552.57001  564.20001  510.26999  563.67999  1244599000\n"
     ]
    }
   ],
   "source": [
    "# Fetch daily time series data for SPY from 2024-01-01 to 2024-12-31\n",
    "ts = td.time_series(\n",
    "    symbol=\"SPY\",\n",
    "    interval=\"1month\",\n",
    "    start_date=\"2024-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    timezone=\"America/New_York\"\n",
    ")\n",
    "\n",
    "# Convert the time series data to a pandas DataFrame\n",
    "df = ts.as_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save the DataFrame to a CSV file\n",
    "df.to_csv(\"data/raw/SPY_2024_monthly.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7492224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 open       high        low      close     volume\n",
      "datetime                                                         \n",
      "2024-12-01  155.91000  182.57001  153.37000  177.00000  238381100\n",
      "2024-11-01  152.78000  157.66000  137.03000  155.44000  269611100\n",
      "2024-10-01  151.46001  163.44000  146.02000  149.31000  339046200\n",
      "2024-09-01  167.03000  169.60001  151.24001  152.03999  191886200\n",
      "2024-08-01  190.00000  191.17000  162.50000  173.74001  128582400\n"
     ]
    }
   ],
   "source": [
    "# Fetch daily time series data for SPY from 2024-01-01 to 2024-12-31\n",
    "ts = td.time_series(\n",
    "    symbol=\"BA\",\n",
    "    interval=\"1month\",\n",
    "    start_date=\"2024-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    timezone=\"America/New_York\"\n",
    ")\n",
    "\n",
    "# Convert the time series data to a pandas DataFrame\n",
    "df = ts.as_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save the DataFrame to a CSV file\n",
    "df.to_csv(\"data/raw/BA_2024_monthly.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6b8d1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 open       high        low   close    volume\n",
      "datetime                                                     \n",
      "2024-12-30  173.72000  178.17000  170.14999  176.55  18082300\n",
      "2024-12-27  180.00999  181.42999  179.39999  180.72   6806900\n",
      "2024-12-26  178.98000  182.57001  178.33000  180.38   5905700\n",
      "2024-12-24  177.69000  180.95000  177.50000  179.34   4317000\n",
      "2024-12-23  178.12000  179.64999  174.28999  177.69   8486400\n"
     ]
    }
   ],
   "source": [
    "# Fetch daily time series data for SPY from 2024-01-01 to 2024-12-31\n",
    "ts = td.time_series(\n",
    "    symbol=\"BA\",\n",
    "    interval=\"1day\",\n",
    "    start_date=\"2024-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    timezone=\"America/New_York\"\n",
    ")\n",
    "\n",
    "# Convert the time series data to a pandas DataFrame\n",
    "df = ts.as_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save the DataFrame to a CSV file\n",
    "df.to_csv(\"data/raw/BA_2024_daily.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eae3f1",
   "metadata": {},
   "source": [
    "# Stockmarket data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f7bce",
   "metadata": {},
   "source": [
    "# Connecting to the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "900386de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'high', 'low', 'close', 'volume'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "504ede7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\nextcloud\\studia - pw\\semestr 6\\hurtownie danych\\marketminer\\marketminer\\.venv\\lib\\site-packages (2.2.3)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp313-cp313-win_amd64.whl.metadata (4.8 kB)\n",
      "Collecting sqlalchemy\n",
      "  Downloading sqlalchemy-2.0.40-cp313-cp313-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\nextcloud\\studia - pw\\semestr 6\\hurtownie danych\\marketminer\\marketminer\\.venv\\lib\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\nextcloud\\studia - pw\\semestr 6\\hurtownie danych\\marketminer\\marketminer\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\nextcloud\\studia - pw\\semestr 6\\hurtownie danych\\marketminer\\marketminer\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\nextcloud\\studia - pw\\semestr 6\\hurtownie danych\\marketminer\\marketminer\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting greenlet>=1 (from sqlalchemy)\n",
      "  Downloading greenlet-3.2.2-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting typing-extensions>=4.6.0 (from sqlalchemy)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\nextcloud\\studia - pw\\semestr 6\\hurtownie danych\\marketminer\\marketminer\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading psycopg2_binary-2.9.10-cp313-cp313-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.6/2.6 MB 15.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 18.3 MB/s eta 0:00:00\n",
      "Downloading sqlalchemy-2.0.40-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 57.4 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.2-cp313-cp313-win_amd64.whl (296 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Installing collected packages: typing-extensions, psycopg2-binary, greenlet, sqlalchemy\n",
      "\n",
      "   -------------------- ------------------- 2/4 [greenlet]\n",
      "   -------------------- ------------------- 2/4 [greenlet]\n",
      "   ------------------------------ --------- 3/4 [sqlalchemy]\n",
      "   ------------------------------ --------- 3/4 [sqlalchemy]\n",
      "   ------------------------------ --------- 3/4 [sqlalchemy]\n",
      "   ------------------------------ --------- 3/4 [sqlalchemy]\n",
      "   ------------------------------ --------- 3/4 [sqlalchemy]\n",
      "   ------------------------------ --------- 3/4 [sqlalchemy]\n",
      "   ------------------------------ --------- 3/4 [sqlalchemy]\n",
      "   ------------------------------ --------- 3/4 [sqlalchemy]\n",
      "   ------------------------------ --------- 3/4 [sqlalchemy]\n",
      "   ------------------------------ --------- 3/4 [sqlalchemy]\n",
      "   ------------------------------ --------- 3/4 [sqlalchemy]\n",
      "   ------------------------------ --------- 3/4 [sqlalchemy]\n",
      "   ------------------------------ --------- 3/4 [sqlalchemy]\n",
      "   ---------------------------------------- 4/4 [sqlalchemy]\n",
      "\n",
      "Successfully installed greenlet-3.2.2 psycopg2-binary-2.9.10 sqlalchemy-2.0.40 typing-extensions-4.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas psycopg2-binary sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1217166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully or already exists.\n",
      "Successfully inserted 30 rows\n",
      "Total rows in table: 30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Database connection parameters\n",
    "db_config = {\n",
    "    'host': '192.168.1.68',\n",
    "    'port': 5432,\n",
    "    'database': 'dwtestdb',\n",
    "    'user': db_username,\n",
    "    'password': db_password\n",
    "}\n",
    "\n",
    "# Add the datetime column to the DataFrame\n",
    "df['datetime'] = df.index\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "try:\n",
    "    connection = psycopg2.connect(**db_config)\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # Create table\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS stock_data (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        open NUMERIC(10, 5),\n",
    "        high NUMERIC(10, 5),\n",
    "        low NUMERIC(10, 5),\n",
    "        close NUMERIC(10, 5),\n",
    "        volume BIGINT,\n",
    "        datetime DATE\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(create_table_query)\n",
    "    connection.commit()\n",
    "    print(\"Table created successfully or already exists.\")\n",
    "    \n",
    "    # Prepare data for insertion\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO stock_data (open, high, low, close, volume, datetime)\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert DataFrame to list of tuples\n",
    "    data_tuples = list(df.itertuples(index=False, name=None))\n",
    "    \n",
    "    # Execute batch insert\n",
    "    execute_values(\n",
    "        cursor,\n",
    "        insert_query,\n",
    "        data_tuples,\n",
    "        template=None,\n",
    "        page_size=100\n",
    "    )\n",
    "    \n",
    "    connection.commit()\n",
    "    print(f\"Successfully inserted {len(data_tuples)} rows\")\n",
    "    \n",
    "    # Verify insertion\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM stock_data\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"Total rows in table: {count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    connection.rollback()\n",
    "    \n",
    "finally:\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408c4d27",
   "metadata": {},
   "source": [
    "# Historical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dcdc0a",
   "metadata": {},
   "source": [
    "Due to the limitations of the free plan, we have to get historical data using other means, we decided Kaggle is the best way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a61f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from kagglehub) (24.2)\n",
      "Collecting pyyaml (from kagglehub)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from requests->kagglehub) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Downloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Installing collected packages: pyyaml, kagglehub\n",
      "Successfully installed kagglehub-0.3.12 pyyaml-6.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee812f",
   "metadata": {},
   "source": [
    "## Inflation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1eb260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\nextcloud\\\\Studia - PW\\\\semestr 6\\\\hurtownie danych\\\\MarketMiner\\\\MarketMiner'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf769dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\admin\\.cache\\kagglehub\\datasets\\sazidthe1\\global-inflation-data\\versions\\1\n",
      "Dataset downloaded and copied to: data/raw/kaggle/inflation_world/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Download latest version\n",
    "dataset_path = kagglehub.dataset_download(\"sazidthe1/global-inflation-data\")\n",
    "kaggle_raw_dir = \"data/raw/kaggle/inflation_world/\"\n",
    "print(\"Path to dataset files:\", dataset_path)\n",
    "\n",
    "if os.path.exists(kaggle_raw_dir):\n",
    "    shutil.rmtree(kaggle_raw_dir)\n",
    "\n",
    "# Copy the entire directory tree\n",
    "shutil.copytree(dataset_path, kaggle_raw_dir)\n",
    "\n",
    "print(f\"Dataset downloaded and copied to: {kaggle_raw_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e533d37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\admin\\.cache\\kagglehub\\datasets\\varpit94\\us-inflation-data-updated-till-may-2021\\versions\\5\n",
      "Dataset downloaded and copied to: /data/raw/kaggle/inflation/\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Download latest version\n",
    "dataset_path = kagglehub.dataset_download(\"varpit94/us-inflation-data-updated-till-may-2021\")\n",
    "kaggle_raw_dir = \"/data/raw/kaggle/inflation/\"\n",
    "print(\"Path to dataset files:\", dataset_path)\n",
    "\n",
    "if os.path.exists(kaggle_raw_dir):\n",
    "    shutil.rmtree(kaggle_raw_dir)\n",
    "\n",
    "# Copy the entire directory tree\n",
    "shutil.copytree(dataset_path, kaggle_raw_dir)\n",
    "\n",
    "print(f\"Dataset downloaded and copied to: {kaggle_raw_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f81e9",
   "metadata": {},
   "source": [
    "## NYSE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9af82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and copied to: /data/raw/kaggle/\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset to a specific directory\n",
    "dataset_path = kagglehub.dataset_download(\"svaningelgem/nyse-100-daily-stock-prices\")\n",
    "\n",
    "# If you want to copy/move it to your desired location\n",
    "kaggle_raw_dir = \"/data/raw/kaggle/nyse/\"\n",
    "\n",
    "# Remove target directory if it exists, then copy everything\n",
    "if os.path.exists(kaggle_raw_dir):\n",
    "    shutil.rmtree(kaggle_raw_dir)\n",
    "\n",
    "# Copy the entire directory tree\n",
    "shutil.copytree(dataset_path, kaggle_raw_dir)\n",
    "\n",
    "print(f\"Dataset downloaded and copied to: {kaggle_raw_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eebb760",
   "metadata": {},
   "source": [
    "## Insert sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22cb1efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 101 files in the directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['ticker', 'date', 'open', 'high', 'low', 'close'], dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(kaggle_raw_dir)\n",
    "\n",
    "print(\"There are\", len(files), \"files in the directory\")\n",
    "\n",
    "# for file in files:\n",
    "#     file_path = os.path.join(kaggle_raw_dir, file)\n",
    "#     print(file, file_path)\n",
    "\n",
    "sample_file = files[0]\n",
    "sample_file_path = os.path.join(kaggle_raw_dir, sample_file)\n",
    "pd.read_csv(sample_file_path).head().columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8381b907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file from: /data/raw/kaggle/ABBV.csv\n",
      "DataFrame shape: (3077, 6)\n",
      "DataFrame columns: Index(['ticker', 'date', 'open', 'high', 'low', 'close'], dtype='object')\n",
      "First 5 rows:\n",
      "  ticker        date     open     high      low    close\n",
      "0   ABBV  2013-01-02  34.9200  35.4000  34.1000  35.1200\n",
      "1   ABBV  2013-01-03  35.0000  35.0000  34.1600  34.8300\n",
      "2   ABBV  2013-01-04  22.9947  23.1732  22.7505  22.8398\n",
      "3   ABBV  2013-01-07  22.6809  23.5459  22.6809  22.8888\n",
      "4   ABBV  2013-01-08  22.7760  23.0095  22.1581  22.3916\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reading CSV file from: {sample_file_path}\")\n",
    "df = pd.read_csv(sample_file_path)\n",
    "\n",
    "# Preview the data\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e75690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 3077 rows into FactStock table\n",
      "PostgreSQL connection closed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Successfully connected to PostgreSQL database\")\n",
    "\n",
    "    # Create the FactStock table\n",
    "    create_table_query = '''\n",
    "    CREATE TABLE IF NOT EXISTS FactStock (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        ticker VARCHAR(10) NOT NULL,\n",
    "        date DATE NOT NULL,\n",
    "        open NUMERIC(10, 2),\n",
    "        high NUMERIC(10, 2),\n",
    "        low NUMERIC(10, 2),\n",
    "        close NUMERIC(10, 2)\n",
    "    );\n",
    "    '''\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print(\"FactStock table created or already exists\")\n",
    "\n",
    "    # Select only the required columns\n",
    "    if set(['ticker', 'date', 'open', 'high', 'low', 'close']).issubset(set(df.columns)):\n",
    "        df_subset = df[['ticker', 'date', 'open', 'high', 'low', 'close']]\n",
    "        \n",
    "        # Convert date column to datetime if it's not already\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df_subset['date']):\n",
    "            df_subset['date'] = pd.to_datetime(df_subset['date'])\n",
    "            \n",
    "        # Convert dataframe to list of tuples for faster insertion\n",
    "        data_tuples = list(df_subset.itertuples(index=False, name=None))\n",
    "        \n",
    "        # Insert the data in batches\n",
    "        insert_query = '''\n",
    "        INSERT INTO FactStock (ticker, date, open, high, low, close)\n",
    "        VALUES %s\n",
    "        '''\n",
    "        \n",
    "        # Use execute_values for faster insertion\n",
    "        execute_values(cursor, insert_query, data_tuples)\n",
    "        conn.commit()\n",
    "        \n",
    "        # Count rows to verify\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM FactStock\")\n",
    "        row_count = cursor.fetchone()[0]\n",
    "        print(f\"Successfully imported {row_count} rows into FactStock table\")\n",
    "    else:\n",
    "        print(\"Error: Required columns not found in the CSV file\")\n",
    "        print(f\"Expected: ['ticker', 'date', 'open', 'high', 'low', 'close']\")\n",
    "        print(f\"Found: {df.columns.tolist()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Database Error: {e}\")\n",
    "    \n",
    "finally:\n",
    "    # Close the connection\n",
    "    if 'conn' in locals() and conn is not None:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"PostgreSQL connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c318fa",
   "metadata": {},
   "source": [
    "## Insert all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de1fab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_stock_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Successfully connected to PostgreSQL database\")\n",
    "\n",
    "        # Create the FactStock table\n",
    "        create_table_query = '''\n",
    "        CREATE TABLE IF NOT EXISTS FactStock (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            ticker VARCHAR(10) NOT NULL,\n",
    "            date DATE NOT NULL,\n",
    "            open NUMERIC(10, 2),\n",
    "            high NUMERIC(10, 2),\n",
    "            low NUMERIC(10, 2),\n",
    "            close NUMERIC(10, 2)\n",
    "        );\n",
    "        '''\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        print(\"FactStock table created or already exists\")\n",
    "\n",
    "        # Select only the required columns\n",
    "        if set(['ticker', 'date', 'open', 'high', 'low', 'close']).issubset(set(df.columns)):\n",
    "            df_subset = df[['ticker', 'date', 'open', 'high', 'low', 'close']]\n",
    "            \n",
    "            # Convert date column to datetime if it's not already\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df_subset['date']):\n",
    "                df_subset['date'] = pd.to_datetime(df_subset['date'])\n",
    "                \n",
    "            # Convert dataframe to list of tuples for faster insertion\n",
    "            data_tuples = list(df_subset.itertuples(index=False, name=None))\n",
    "            \n",
    "            # Insert the data in batches\n",
    "            insert_query = '''\n",
    "            INSERT INTO FactStock (ticker, date, open, high, low, close)\n",
    "            VALUES %s\n",
    "            '''\n",
    "            \n",
    "            # Use execute_values for faster insertion\n",
    "            execute_values(cursor, insert_query, data_tuples)\n",
    "            conn.commit()\n",
    "            \n",
    "            # Count rows to verify\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM FactStock\")\n",
    "            row_count = cursor.fetchone()[0]\n",
    "            print(f\"Successfully imported {row_count} rows into FactStock table\")\n",
    "        else:\n",
    "            print(\"Error: Required columns not found in the CSV file\")\n",
    "            print(f\"Expected: ['ticker', 'date', 'open', 'high', 'low', 'close']\")\n",
    "            print(f\"Found: {df.columns.tolist()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Database Error: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        # Close the connection\n",
    "        if 'conn' in locals() and conn is not None:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"PostgreSQL connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88f5c7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ABBV.csv...\n",
      "ABBV.csv /data/raw/kaggle/ABBV.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 225910 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing ABT.csv...\n",
      "ABT.csv /data/raw/kaggle/ABT.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 237261 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing ACN.csv...\n",
      "ACN.csv /data/raw/kaggle/ACN.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 243219 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing AMT.csv...\n",
      "AMT.csv /data/raw/kaggle/AMT.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 250032 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing ANET.csv...\n",
      "ANET.csv /data/raw/kaggle/ANET.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 252751 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing APH.csv...\n",
      "APH.csv /data/raw/kaggle/APH.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 261156 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing AXP.csv...\n",
      "AXP.csv /data/raw/kaggle/AXP.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 274566 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing BA.csv...\n",
      "BA.csv /data/raw/kaggle/BA.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 290482 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing BABA.csv...\n",
      "BABA.csv /data/raw/kaggle/BABA.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 293128 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing BAC.csv...\n",
      "BAC.csv /data/raw/kaggle/BAC.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 306265 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing BHP.csv...\n",
      "BHP.csv /data/raw/kaggle/BHP.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 317616 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing BLK.csv...\n",
      "BLK.csv /data/raw/kaggle/BLK.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 324027 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing BMY.csv...\n",
      "BMY.csv /data/raw/kaggle/BMY.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 337344 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing BRK.B.csv...\n",
      "BRK.B.csv /data/raw/kaggle/BRK.B.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 344612 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing BSX.csv...\n",
      "BSX.csv /data/raw/kaggle/BSX.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 352885 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing BTI.csv...\n",
      "BTI.csv /data/raw/kaggle/BTI.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 364217 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing BUD.csv...\n",
      "BUD.csv /data/raw/kaggle/BUD.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 368177 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing BX.csv...\n",
      "BX.csv /data/raw/kaggle/BX.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 372648 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing C.csv...\n",
      "C.csv /data/raw/kaggle/C.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 386576 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing CAT.csv...\n",
      "CAT.csv /data/raw/kaggle/CAT.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 402492 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing CB.csv...\n",
      "CB.csv /data/raw/kaggle/CB.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 410550 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing COP.csv...\n",
      "COP.csv /data/raw/kaggle/COP.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 421448 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing CRM.csv...\n",
      "CRM.csv /data/raw/kaggle/CRM.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 426673 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing CVX.csv...\n",
      "CVX.csv /data/raw/kaggle/CVX.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 442589 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing DE.csv...\n",
      "DE.csv /data/raw/kaggle/DE.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 455906 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing DHR.csv...\n",
      "DHR.csv /data/raw/kaggle/DHR.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 467563 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing DIS.csv...\n",
      "DIS.csv /data/raw/kaggle/DIS.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 483479 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing DUK.csv...\n",
      "DUK.csv /data/raw/kaggle/DUK.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 494830 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing ELV.csv...\n",
      "ELV.csv /data/raw/kaggle/ELV.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 502925 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing ENB.csv...\n",
      "ENB.csv /data/raw/kaggle/ENB.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 513265 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing ETN.csv...\n",
      "ETN.csv /data/raw/kaggle/ETN.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 526582 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing FI.csv...\n",
      "FI.csv /data/raw/kaggle/FI.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 535398 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing GE.csv...\n",
      "GE.csv /data/raw/kaggle/GE.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 551314 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing GEV.csv...\n",
      "GEV.csv /data/raw/kaggle/GEV.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 551561 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing GS.csv...\n",
      "GS.csv /data/raw/kaggle/GS.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 558077 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing HD.csv...\n",
      "HD.csv /data/raw/kaggle/HD.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 569045 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing HDB.csv...\n",
      "HDB.csv /data/raw/kaggle/HDB.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 575002 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing HSBC.csv...\n",
      "HSBC.csv /data/raw/kaggle/HSBC.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 581467 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing IBM.csv...\n",
      "IBM.csv /data/raw/kaggle/IBM.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 597383 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing IBN.csv...\n",
      "IBN.csv /data/raw/kaggle/IBN.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 603671 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing ICE.csv...\n",
      "ICE.csv /data/raw/kaggle/ICE.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 608542 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing JNJ.csv...\n",
      "JNJ.csv /data/raw/kaggle/JNJ.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 624458 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing JPM.csv...\n",
      "JPM.csv /data/raw/kaggle/JPM.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 638386 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing KKR.csv...\n",
      "KKR.csv /data/raw/kaggle/KKR.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 642086 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing KO.csv...\n",
      "KO.csv /data/raw/kaggle/KO.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 658002 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing LLY.csv...\n",
      "LLY.csv /data/raw/kaggle/LLY.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 671319 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing LMT.csv...\n",
      "LMT.csv /data/raw/kaggle/LMT.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 683479 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing LOW.csv...\n",
      "LOW.csv /data/raw/kaggle/LOW.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 694830 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing MA.csv...\n",
      "MA.csv /data/raw/kaggle/MA.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 699571 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing MCD.csv...\n",
      "MCD.csv /data/raw/kaggle/MCD.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 714352 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing MDT.csv...\n",
      "MDT.csv /data/raw/kaggle/MDT.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 727440 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing MMC.csv...\n",
      "MMC.csv /data/raw/kaggle/MMC.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 740577 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing MO.csv...\n",
      "MO.csv /data/raw/kaggle/MO.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 756493 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing MRK.csv...\n",
      "MRK.csv /data/raw/kaggle/MRK.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 772409 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing MS.csv...\n",
      "MS.csv /data/raw/kaggle/MS.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 780489 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing MUFG.csv...\n",
      "MUFG.csv /data/raw/kaggle/MUFG.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 786522 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing NEE.csv...\n",
      "NEE.csv /data/raw/kaggle/NEE.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 799659 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing NOW.csv...\n",
      "NOW.csv /data/raw/kaggle/NOW.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 802863 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing NVO.csv...\n",
      "NVO.csv /data/raw/kaggle/NVO.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 813931 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing NVS.csv...\n",
      "NVS.csv /data/raw/kaggle/NVS.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 821072 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing ORCL.csv...\n",
      "ORCL.csv /data/raw/kaggle/ORCL.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 830910 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing PFE.csv...\n",
      "PFE.csv /data/raw/kaggle/PFE.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 844227 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing PG.csv...\n",
      "PG.csv /data/raw/kaggle/PG.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 860143 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing PGR.csv...\n",
      "PGR.csv /data/raw/kaggle/PGR.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 871494 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing PLD.csv...\n",
      "PLD.csv /data/raw/kaggle/PLD.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 878945 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing PM.csv...\n",
      "PM.csv /data/raw/kaggle/PM.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 883231 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing RELX.csv...\n",
      "RELX.csv /data/raw/kaggle/RELX.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 890901 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing RIO.csv...\n",
      "RIO.csv /data/raw/kaggle/RIO.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 899652 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing RTX.csv...\n",
      "RTX.csv /data/raw/kaggle/RTX.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 915505 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing RY.csv...\n",
      "RY.csv /data/raw/kaggle/RY.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 922916 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing SAN.csv...\n",
      "SAN.csv /data/raw/kaggle/SAN.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 932404 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing SAP.csv...\n",
      "SAP.csv /data/raw/kaggle/SAP.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 939835 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing SCHW.csv...\n",
      "SCHW.csv /data/raw/kaggle/SCHW.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 949286 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing SHEL.csv...\n",
      "SHEL.csv /data/raw/kaggle/SHEL.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 956939 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing SHW.csv...\n",
      "SHW.csv /data/raw/kaggle/SHW.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 968290 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing SO.csv...\n",
      "SO.csv /data/raw/kaggle/SO.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 979188 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing SONY.csv...\n",
      "SONY.csv /data/raw/kaggle/SONY.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 992325 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing SPGI.csv...\n",
      "SPGI.csv /data/raw/kaggle/SPGI.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1005462 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing SPOT.csv...\n",
      "SPOT.csv /data/raw/kaggle/SPOT.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1007219 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing SYK.csv...\n",
      "SYK.csv /data/raw/kaggle/SYK.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1018570 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing T.csv...\n",
      "T.csv /data/raw/kaggle/T.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1028989 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing TD.csv...\n",
      "TD.csv /data/raw/kaggle/TD.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1036178 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing TJX.csv...\n",
      "TJX.csv /data/raw/kaggle/TJX.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1045689 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing TM.csv...\n",
      "TM.csv /data/raw/kaggle/TM.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1057040 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing TMO.csv...\n",
      "TMO.csv /data/raw/kaggle/TMO.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1068391 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing TSM.csv...\n",
      "TSM.csv /data/raw/kaggle/TSM.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1075300 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing TT.csv...\n",
      "TT.csv /data/raw/kaggle/TT.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1086651 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing TTE.csv...\n",
      "TTE.csv /data/raw/kaggle/TTE.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1095066 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing UBER.csv...\n",
      "UBER.csv /data/raw/kaggle/UBER.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1096545 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing UBS.csv...\n",
      "UBS.csv /data/raw/kaggle/UBS.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1102799 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing UL.csv...\n",
      "UL.csv /data/raw/kaggle/UL.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1114144 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing UNH.csv...\n",
      "UNH.csv /data/raw/kaggle/UNH.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1124334 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing UNP.csv...\n",
      "UNP.csv /data/raw/kaggle/UNP.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1135737 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing V.csv...\n",
      "V.csv /data/raw/kaggle/V.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1140022 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing VZ.csv...\n",
      "VZ.csv /data/raw/kaggle/VZ.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1150441 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing WELL.csv...\n",
      "WELL.csv /data/raw/kaggle/WELL.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1161790 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing WFC.csv...\n",
      "WFC.csv /data/raw/kaggle/WFC.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1175107 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing WM.csv...\n",
      "WM.csv /data/raw/kaggle/WM.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1184368 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing WMT.csv...\n",
      "WMT.csv /data/raw/kaggle/WMT.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1197718 rows into FactStock table\n",
      "PostgreSQL connection closed\n",
      "Processing XOM.csv...\n",
      "XOM.csv /data/raw/kaggle/XOM.csv\n",
      "Successfully connected to PostgreSQL database\n",
      "FactStock table created or already exists\n",
      "Successfully imported 1213634 rows into FactStock table\n",
      "PostgreSQL connection closed\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        # Check if the file is a CSV file\n",
    "        print(f\"Processing {file}...\")\n",
    "        file_path = os.path.join(kaggle_raw_dir, file)\n",
    "        print(file, file_path)\n",
    "        insert_stock_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e385b",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac6d7a",
   "metadata": {},
   "source": [
    "## Get stock data for month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4f72788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(symbol, interval, start_date, end_date, custom_name=None):\n",
    "    \"\"\"\n",
    "    Fetch stock data from Twelve Data API.\n",
    "    \n",
    "    Parameters:\n",
    "    symbol (str): Stock symbol.\n",
    "    interval (str): Time interval (e.g., '1min', '1day').\n",
    "    start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "    end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing stock data.\n",
    "    \"\"\"\n",
    "    ts = td.time_series(\n",
    "        symbol=symbol,\n",
    "        interval=interval,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        timezone=\"America/New_York\"\n",
    "    )\n",
    "\n",
    "    df = ts.as_pandas()\n",
    "\n",
    "    file_name = f\"data/raw/{symbol}_{interval}_{start_date}_{end_date}.csv\"\n",
    "    if custom_name:\n",
    "        file_name = f\"data/raw/{custom_name}.csv\"\n",
    "\n",
    "    df.to_csv(file_name, index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfbd62f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-02-27</th>\n",
       "      <td>200.92999</td>\n",
       "      <td>202.00000</td>\n",
       "      <td>200.03999</td>\n",
       "      <td>201.39999</td>\n",
       "      <td>3932900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-26</th>\n",
       "      <td>201.00999</td>\n",
       "      <td>202.75000</td>\n",
       "      <td>200.00000</td>\n",
       "      <td>200.53999</td>\n",
       "      <td>4807200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-23</th>\n",
       "      <td>200.99001</td>\n",
       "      <td>202.07001</td>\n",
       "      <td>197.14999</td>\n",
       "      <td>200.83000</td>\n",
       "      <td>7433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-22</th>\n",
       "      <td>202.00000</td>\n",
       "      <td>204.10001</td>\n",
       "      <td>200.39999</td>\n",
       "      <td>201.50000</td>\n",
       "      <td>6513500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-21</th>\n",
       "      <td>202.89999</td>\n",
       "      <td>203.63000</td>\n",
       "      <td>201.21001</td>\n",
       "      <td>201.57001</td>\n",
       "      <td>4179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-20</th>\n",
       "      <td>203.55000</td>\n",
       "      <td>205.58000</td>\n",
       "      <td>202.71001</td>\n",
       "      <td>203.37000</td>\n",
       "      <td>5248400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-16</th>\n",
       "      <td>204.88000</td>\n",
       "      <td>205.05000</td>\n",
       "      <td>202.81000</td>\n",
       "      <td>203.89000</td>\n",
       "      <td>5975900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-15</th>\n",
       "      <td>204.12000</td>\n",
       "      <td>206.55000</td>\n",
       "      <td>203.39999</td>\n",
       "      <td>205.33000</td>\n",
       "      <td>5309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-14</th>\n",
       "      <td>205.95000</td>\n",
       "      <td>206.58000</td>\n",
       "      <td>202.21001</td>\n",
       "      <td>203.38000</td>\n",
       "      <td>6503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-13</th>\n",
       "      <td>206.89999</td>\n",
       "      <td>207.81000</td>\n",
       "      <td>202.33000</td>\n",
       "      <td>204.46001</td>\n",
       "      <td>8263600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-12</th>\n",
       "      <td>208.70000</td>\n",
       "      <td>209.80000</td>\n",
       "      <td>207.60001</td>\n",
       "      <td>209.33000</td>\n",
       "      <td>4077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-09</th>\n",
       "      <td>209.77000</td>\n",
       "      <td>211.44000</td>\n",
       "      <td>207.89999</td>\n",
       "      <td>209.20000</td>\n",
       "      <td>4349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-08</th>\n",
       "      <td>212.39999</td>\n",
       "      <td>213.10001</td>\n",
       "      <td>208.72000</td>\n",
       "      <td>209.22000</td>\n",
       "      <td>5787700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-07</th>\n",
       "      <td>208.75999</td>\n",
       "      <td>213.77000</td>\n",
       "      <td>207.99001</td>\n",
       "      <td>211.92000</td>\n",
       "      <td>7747700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-06</th>\n",
       "      <td>206.02000</td>\n",
       "      <td>210.64000</td>\n",
       "      <td>203.89000</td>\n",
       "      <td>208.58000</td>\n",
       "      <td>7556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-05</th>\n",
       "      <td>204.64000</td>\n",
       "      <td>207.34000</td>\n",
       "      <td>202.00000</td>\n",
       "      <td>206.63000</td>\n",
       "      <td>8682500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-02</th>\n",
       "      <td>209.06000</td>\n",
       "      <td>209.53999</td>\n",
       "      <td>206.35001</td>\n",
       "      <td>209.38000</td>\n",
       "      <td>6271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-01</th>\n",
       "      <td>213.84000</td>\n",
       "      <td>214.63000</td>\n",
       "      <td>206.81000</td>\n",
       "      <td>209.81000</td>\n",
       "      <td>10679200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-31</th>\n",
       "      <td>204.92000</td>\n",
       "      <td>213.77000</td>\n",
       "      <td>203.25000</td>\n",
       "      <td>211.03999</td>\n",
       "      <td>22409400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-30</th>\n",
       "      <td>203.64999</td>\n",
       "      <td>203.98000</td>\n",
       "      <td>199.14000</td>\n",
       "      <td>200.44000</td>\n",
       "      <td>13321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-29</th>\n",
       "      <td>206.06000</td>\n",
       "      <td>207.03999</td>\n",
       "      <td>203.85001</td>\n",
       "      <td>205.19000</td>\n",
       "      <td>7535400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-26</th>\n",
       "      <td>203.08000</td>\n",
       "      <td>206.75000</td>\n",
       "      <td>203.00000</td>\n",
       "      <td>205.47000</td>\n",
       "      <td>9911200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-25</th>\n",
       "      <td>208.20000</td>\n",
       "      <td>208.73000</td>\n",
       "      <td>198.32001</td>\n",
       "      <td>201.88000</td>\n",
       "      <td>22112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-24</th>\n",
       "      <td>209.83000</td>\n",
       "      <td>217.59000</td>\n",
       "      <td>209.36000</td>\n",
       "      <td>214.13000</td>\n",
       "      <td>15119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-23</th>\n",
       "      <td>215.35001</td>\n",
       "      <td>216.85001</td>\n",
       "      <td>211.13000</td>\n",
       "      <td>211.50000</td>\n",
       "      <td>9076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-22</th>\n",
       "      <td>213.07001</td>\n",
       "      <td>217.00000</td>\n",
       "      <td>212.36000</td>\n",
       "      <td>214.92999</td>\n",
       "      <td>10760800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-19</th>\n",
       "      <td>210.89000</td>\n",
       "      <td>215.16000</td>\n",
       "      <td>209.23000</td>\n",
       "      <td>215.02000</td>\n",
       "      <td>14320200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-18</th>\n",
       "      <td>205.64000</td>\n",
       "      <td>213.03999</td>\n",
       "      <td>203.32001</td>\n",
       "      <td>211.61000</td>\n",
       "      <td>20046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17</th>\n",
       "      <td>202.63000</td>\n",
       "      <td>206.30000</td>\n",
       "      <td>201.64999</td>\n",
       "      <td>203.06000</td>\n",
       "      <td>20140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-16</th>\n",
       "      <td>210.07001</td>\n",
       "      <td>210.98000</td>\n",
       "      <td>199.50000</td>\n",
       "      <td>200.52000</td>\n",
       "      <td>35290000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 open       high        low      close    volume\n",
       "datetime                                                        \n",
       "2024-02-27  200.92999  202.00000  200.03999  201.39999   3932900\n",
       "2024-02-26  201.00999  202.75000  200.00000  200.53999   4807200\n",
       "2024-02-23  200.99001  202.07001  197.14999  200.83000   7433400\n",
       "2024-02-22  202.00000  204.10001  200.39999  201.50000   6513500\n",
       "2024-02-21  202.89999  203.63000  201.21001  201.57001   4179800\n",
       "2024-02-20  203.55000  205.58000  202.71001  203.37000   5248400\n",
       "2024-02-16  204.88000  205.05000  202.81000  203.89000   5975900\n",
       "2024-02-15  204.12000  206.55000  203.39999  205.33000   5309100\n",
       "2024-02-14  205.95000  206.58000  202.21001  203.38000   6503000\n",
       "2024-02-13  206.89999  207.81000  202.33000  204.46001   8263600\n",
       "2024-02-12  208.70000  209.80000  207.60001  209.33000   4077400\n",
       "2024-02-09  209.77000  211.44000  207.89999  209.20000   4349700\n",
       "2024-02-08  212.39999  213.10001  208.72000  209.22000   5787700\n",
       "2024-02-07  208.75999  213.77000  207.99001  211.92000   7747700\n",
       "2024-02-06  206.02000  210.64000  203.89000  208.58000   7556900\n",
       "2024-02-05  204.64000  207.34000  202.00000  206.63000   8682500\n",
       "2024-02-02  209.06000  209.53999  206.35001  209.38000   6271200\n",
       "2024-02-01  213.84000  214.63000  206.81000  209.81000  10679200\n",
       "2024-01-31  204.92000  213.77000  203.25000  211.03999  22409400\n",
       "2024-01-30  203.64999  203.98000  199.14000  200.44000  13321200\n",
       "2024-01-29  206.06000  207.03999  203.85001  205.19000   7535400\n",
       "2024-01-26  203.08000  206.75000  203.00000  205.47000   9911200\n",
       "2024-01-25  208.20000  208.73000  198.32001  201.88000  22112500\n",
       "2024-01-24  209.83000  217.59000  209.36000  214.13000  15119100\n",
       "2024-01-23  215.35001  216.85001  211.13000  211.50000   9076900\n",
       "2024-01-22  213.07001  217.00000  212.36000  214.92999  10760800\n",
       "2024-01-19  210.89000  215.16000  209.23000  215.02000  14320200\n",
       "2024-01-18  205.64000  213.03999  203.32001  211.61000  20046800\n",
       "2024-01-17  202.63000  206.30000  201.64999  203.06000  20140100\n",
       "2024-01-16  210.07001  210.98000  199.50000  200.52000  35290000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stock_data(\"BA\", \"1day\", \"2024-01-01\", \"2024-02-28\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd244b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch daily time series data for SPY from 2024-01-01 to 2024-12-31\n",
    "ts = td.time_series(\n",
    "    symbol=\"BA\",\n",
    "    interval=\"1day\",\n",
    "    start_date=\"2024-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    timezone=\"America/New_York\"\n",
    ")\n",
    "\n",
    "# Convert the time series data to a pandas DataFrame\n",
    "df = ts.as_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save the DataFrame to a CSV file\n",
    "df.to_csv(\"data/raw/BA_2024_daily.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07dbaf0",
   "metadata": {},
   "source": [
    "## Insert stock data into db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
