{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe5ed1d1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0407469",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:49:29.718238Z",
     "start_time": "2025-05-19T00:49:29.011901Z"
    }
   },
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "dc0bdbff",
   "metadata": {},
   "source": [
    "# Data from TwelveData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f8435",
   "metadata": {},
   "source": [
    "First you have to create an account [here](https://twelvedata.com) and the generate a key [here](https://twelvedata.com/account/api-keys). Place the key in a file named .env and make sure there exists this field:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186bda1",
   "metadata": {},
   "source": [
    "> ⚠️ **Important:** Do **not** share this key or commit the `.env` file to version control. It's already excluded in `.gitignore`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b50af",
   "metadata": {},
   "source": [
    "> TD_API_KEY=<yllaertubyrtecinreadbackwards>"
   ]
  },
  {
   "cell_type": "code",
   "id": "3d24e309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:49:29.752270Z",
     "start_time": "2025-05-19T00:49:29.719961Z"
    }
   },
   "source": [
    "from twelvedata import TDClient\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "ed5aae2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:49:29.756214Z",
     "start_time": "2025-05-19T00:49:29.752270Z"
    }
   },
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"TD_API_KEY\")\n",
    "db_username = os.getenv(\"db_username\")\n",
    "db_password = os.getenv(\"db_password\")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "5dcae8a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:49:32.188559Z",
     "start_time": "2025-05-19T00:49:29.756214Z"
    }
   },
   "source": [
    "# Initialize client\n",
    "td = TDClient(apikey=api_key)\n",
    "\n",
    "# Fetch data\n",
    "ts = td.time_series(\n",
    "    symbol=\"AAPL\",\n",
    "    interval=\"1min\",\n",
    "    outputsize=100,\n",
    "    timezone=\"America/New_York\",\n",
    ")\n",
    "\n",
    "df = ts.as_pandas()\n",
    "print(df.head())\n"
   ],
   "outputs": [
    {
     "ename": "InvalidApiKeyError",
     "evalue": "**apikey** parameter is incorrect or not specified. You can get your free API key instantly following this link: https://twelvedata.com/pricing. If you believe that everything is correct, you can contact us at https://twelvedata.com/contact/customer",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mInvalidApiKeyError\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Fetch data\u001B[39;00m\n\u001B[32m      5\u001B[39m ts = td.time_series(\n\u001B[32m      6\u001B[39m     symbol=\u001B[33m\"\u001B[39m\u001B[33mAAPL\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      7\u001B[39m     interval=\u001B[33m\"\u001B[39m\u001B[33m1min\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      8\u001B[39m     outputsize=\u001B[32m100\u001B[39m,\n\u001B[32m      9\u001B[39m     timezone=\u001B[33m\"\u001B[39m\u001B[33mAmerica/New_York\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     10\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m df = \u001B[43mts\u001B[49m\u001B[43m.\u001B[49m\u001B[43mas_pandas\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[38;5;28mprint\u001B[39m(df.head())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MarketMiner\\.venv\\Lib\\site-packages\\twelvedata\\time_series.py:114\u001B[39m, in \u001B[36mTimeSeries.as_pandas\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m    111\u001B[39m postfixes = \u001B[38;5;28mself\u001B[39m._generate_postfixes()\n\u001B[32m    113\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.price_endpoint_enabled:\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m     df = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mprice_endpoint\u001B[49m\u001B[43m.\u001B[49m\u001B[43mas_pandas\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    116\u001B[39m     df = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MarketMiner\\.venv\\Lib\\site-packages\\twelvedata\\mixins.py:47\u001B[39m, in \u001B[36mAsPandasMixin.as_pandas\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mas_json\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mas_json\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     48\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mis_batch\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.is_batch:\n\u001B[32m     49\u001B[39m     df = convert_collection_to_pandas_multi_index(data)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MarketMiner\\.venv\\Lib\\site-packages\\twelvedata\\mixins.py:12\u001B[39m, in \u001B[36mAsJsonMixin.as_json\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mas_json\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m     resp = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mJSON\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m     json = resp.json()\n\u001B[32m     14\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mis_batch\u001B[39m\u001B[33m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.is_batch:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MarketMiner\\.venv\\Lib\\site-packages\\twelvedata\\endpoints.py:319\u001B[39m, in \u001B[36mTimeSeriesEndpoint.execute\u001B[39m\u001B[34m(self, format, debug)\u001B[39m\n\u001B[32m    317\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m debug:\n\u001B[32m    318\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m build_url(\u001B[38;5;28mself\u001B[39m.ctx.base_url, endpoint, params)\n\u001B[32m--> \u001B[39m\u001B[32m319\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhttp_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MarketMiner\\.venv\\Lib\\site-packages\\twelvedata\\http_client.py:51\u001B[39m, in \u001B[36mDefaultHttpClient.get\u001B[39m\u001B[34m(self, relative_url, *args, **kwargs)\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[32m     49\u001B[39m     message = resp.text\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_raise_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MarketMiner\\.venv\\Lib\\site-packages\\twelvedata\\http_client.py:56\u001B[39m, in \u001B[36mDefaultHttpClient._raise_error\u001B[39m\u001B[34m(error_code, message)\u001B[39m\n\u001B[32m     53\u001B[39m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[32m     54\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_raise_error\u001B[39m(error_code, message):\n\u001B[32m     55\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m error_code == \u001B[32m401\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m56\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidApiKeyError(message)\n\u001B[32m     58\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m error_code == \u001B[32m400\u001B[39m:\n\u001B[32m     59\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m BadRequestError(message)\n",
      "\u001B[31mInvalidApiKeyError\u001B[39m: **apikey** parameter is incorrect or not specified. You can get your free API key instantly following this link: https://twelvedata.com/pricing. If you believe that everything is correct, you can contact us at https://twelvedata.com/contact/customer"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "e096e4e3",
   "metadata": {},
   "source": [
    "df.reset_index().to_csv(\"data/raw/AAPL_1min.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb333a93",
   "metadata": {},
   "source": [
    "# Fetch daily time series data for SPY from 2024-01-01 to 2024-12-31\n",
    "ts = td.time_series(\n",
    "    symbol=\"SPY\",\n",
    "    interval=\"1day\",\n",
    "    start_date=\"2024-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    timezone=\"America/New_York\"\n",
    ")\n",
    "\n",
    "# Convert the time series data to a pandas DataFrame\n",
    "df = ts.as_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save the DataFrame to a CSV file\n",
    "df.to_csv(\"data/raw/SPY_2024_daily.csv\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "176fdffd",
   "metadata": {},
   "source": [
    "# Fetch daily time series data for SPY from 2024-01-01 to 2024-12-31\n",
    "ts = td.time_series(\n",
    "    symbol=\"SPY\",\n",
    "    interval=\"1month\",\n",
    "    start_date=\"2024-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    timezone=\"America/New_York\"\n",
    ")\n",
    "\n",
    "# Convert the time series data to a pandas DataFrame\n",
    "df = ts.as_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save the DataFrame to a CSV file\n",
    "df.to_csv(\"data/raw/SPY_2024_monthly.csv\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7492224a",
   "metadata": {},
   "source": [
    "# Fetch daily time series data for SPY from 2024-01-01 to 2024-12-31\n",
    "ts = td.time_series(\n",
    "    symbol=\"BA\",\n",
    "    interval=\"1month\",\n",
    "    start_date=\"2024-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    timezone=\"America/New_York\"\n",
    ")\n",
    "\n",
    "# Convert the time series data to a pandas DataFrame\n",
    "df = ts.as_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save the DataFrame to a CSV file\n",
    "df.to_csv(\"data/raw/BA_2024_monthly.csv\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b6b8d1bd",
   "metadata": {},
   "source": [
    "# Fetch daily time series data for SPY from 2024-01-01 to 2024-12-31\n",
    "ts = td.time_series(\n",
    "    symbol=\"BA\",\n",
    "    interval=\"1day\",\n",
    "    start_date=\"2024-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    timezone=\"America/New_York\"\n",
    ")\n",
    "\n",
    "# Convert the time series data to a pandas DataFrame\n",
    "df = ts.as_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save the DataFrame to a CSV file\n",
    "df.to_csv(\"data/raw/BA_2024_daily.csv\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b0eae3f1",
   "metadata": {},
   "source": [
    "# Stockmarket data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f7bce",
   "metadata": {},
   "source": [
    "# Connecting to the DB"
   ]
  },
  {
   "cell_type": "code",
   "id": "900386de",
   "metadata": {},
   "source": [
    "df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "504ede7e",
   "metadata": {},
   "source": "#pip install pandas psycopg2-binary sqlalchemy",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d1217166",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Database connection parameters\n",
    "db_config = {\n",
    "    'host': '192.168.1.68',\n",
    "    'port': 5432,\n",
    "    'database': 'dwtestdb',\n",
    "    'user': db_username,\n",
    "    'password': db_password\n",
    "}\n",
    "\n",
    "# Add the datetime column to the DataFrame\n",
    "df['datetime'] = df.index\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "try:\n",
    "    connection = psycopg2.connect(**db_config)\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # Create table\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS stock_data (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        open NUMERIC(10, 5),\n",
    "        high NUMERIC(10, 5),\n",
    "        low NUMERIC(10, 5),\n",
    "        close NUMERIC(10, 5),\n",
    "        volume BIGINT,\n",
    "        datetime DATE\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(create_table_query)\n",
    "    connection.commit()\n",
    "    print(\"Table created successfully or already exists.\")\n",
    "    \n",
    "    # Prepare data for insertion\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO stock_data (open, high, low, close, volume, datetime)\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert DataFrame to list of tuples\n",
    "    data_tuples = list(df.itertuples(index=False, name=None))\n",
    "    \n",
    "    # Execute batch insert\n",
    "    execute_values(\n",
    "        cursor,\n",
    "        insert_query,\n",
    "        data_tuples,\n",
    "        template=None,\n",
    "        page_size=100\n",
    "    )\n",
    "    \n",
    "    connection.commit()\n",
    "    print(f\"Successfully inserted {len(data_tuples)} rows\")\n",
    "    \n",
    "    # Verify insertion\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM stock_data\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"Total rows in table: {count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    connection.rollback()\n",
    "    \n",
    "finally:\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "408c4d27",
   "metadata": {},
   "source": [
    "# Historical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dcdc0a",
   "metadata": {},
   "source": [
    "Due to the limitations of the free plan, we have to get historical data using other means, we decided Kaggle is the best way."
   ]
  },
  {
   "cell_type": "code",
   "id": "88a61f29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:49:32.197751Z",
     "start_time": "2025-05-19T00:49:32.197751Z"
    }
   },
   "source": "#pip install kagglehub",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1ee812f",
   "metadata": {},
   "source": [
    "## Inflation data"
   ]
  },
  {
   "cell_type": "code",
   "id": "8b1eb260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T00:49:32.199760Z",
     "start_time": "2025-05-19T00:49:32.197751Z"
    }
   },
   "source": [
    "os.getcwd()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3bf769dd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Download latest version\n",
    "dataset_path = kagglehub.dataset_download(\"sazidthe1/global-inflation-data\")\n",
    "kaggle_raw_dir = \"data/raw/kaggle/inflation_world/\"\n",
    "print(\"Path to dataset files:\", dataset_path)\n",
    "\n",
    "if os.path.exists(kaggle_raw_dir):\n",
    "    shutil.rmtree(kaggle_raw_dir)\n",
    "\n",
    "# Copy the entire directory tree\n",
    "shutil.copytree(dataset_path, kaggle_raw_dir)\n",
    "\n",
    "print(f\"Dataset downloaded and copied to: {kaggle_raw_dir}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e533d37a",
   "metadata": {},
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Download latest version\n",
    "dataset_path = kagglehub.dataset_download(\"varpit94/us-inflation-data-updated-till-may-2021\")\n",
    "kaggle_raw_dir = \"/data/raw/kaggle/inflation/\"\n",
    "print(\"Path to dataset files:\", dataset_path)\n",
    "\n",
    "if os.path.exists(kaggle_raw_dir):\n",
    "    shutil.rmtree(kaggle_raw_dir)\n",
    "\n",
    "# Copy the entire directory tree\n",
    "shutil.copytree(dataset_path, kaggle_raw_dir)\n",
    "\n",
    "print(f\"Dataset downloaded and copied to: {kaggle_raw_dir}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "506f81e9",
   "metadata": {},
   "source": [
    "## NYSE data"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f9af82c",
   "metadata": {},
   "source": [
    "# Download the dataset to a specific directory\n",
    "dataset_path = kagglehub.dataset_download(\"svaningelgem/nyse-100-daily-stock-prices\")\n",
    "\n",
    "# If you want to copy/move it to your desired location\n",
    "kaggle_raw_dir = \"/data/raw/kaggle/nyse/\"\n",
    "\n",
    "# Remove target directory if it exists, then copy everything\n",
    "if os.path.exists(kaggle_raw_dir):\n",
    "    shutil.rmtree(kaggle_raw_dir)\n",
    "\n",
    "# Copy the entire directory tree\n",
    "shutil.copytree(dataset_path, kaggle_raw_dir)\n",
    "\n",
    "print(f\"Dataset downloaded and copied to: {kaggle_raw_dir}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9eebb760",
   "metadata": {},
   "source": [
    "## Insert sample"
   ]
  },
  {
   "cell_type": "code",
   "id": "22cb1efd",
   "metadata": {},
   "source": [
    "files = os.listdir(kaggle_raw_dir)\n",
    "\n",
    "print(\"There are\", len(files), \"files in the directory\")\n",
    "\n",
    "# for file in files:\n",
    "#     file_path = os.path.join(kaggle_raw_dir, file)\n",
    "#     print(file, file_path)\n",
    "\n",
    "sample_file = files[0]\n",
    "sample_file_path = os.path.join(kaggle_raw_dir, sample_file)\n",
    "pd.read_csv(sample_file_path).head().columns\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8381b907",
   "metadata": {},
   "source": [
    "print(f\"Reading CSV file from: {sample_file_path}\")\n",
    "df = pd.read_csv(sample_file_path)\n",
    "\n",
    "# Preview the data\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40e75690",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Successfully connected to PostgreSQL database\")\n",
    "\n",
    "    # Create the FactStock table\n",
    "    create_table_query = '''\n",
    "    CREATE TABLE IF NOT EXISTS FactStock (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        ticker VARCHAR(10) NOT NULL,\n",
    "        date DATE NOT NULL,\n",
    "        open NUMERIC(10, 2),\n",
    "        high NUMERIC(10, 2),\n",
    "        low NUMERIC(10, 2),\n",
    "        close NUMERIC(10, 2)\n",
    "    );\n",
    "    '''\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print(\"FactStock table created or already exists\")\n",
    "\n",
    "    # Select only the required columns\n",
    "    if set(['ticker', 'date', 'open', 'high', 'low', 'close']).issubset(set(df.columns)):\n",
    "        df_subset = df[['ticker', 'date', 'open', 'high', 'low', 'close']]\n",
    "        \n",
    "        # Convert date column to datetime if it's not already\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df_subset['date']):\n",
    "            df_subset['date'] = pd.to_datetime(df_subset['date'])\n",
    "            \n",
    "        # Convert dataframe to list of tuples for faster insertion\n",
    "        data_tuples = list(df_subset.itertuples(index=False, name=None))\n",
    "        \n",
    "        # Insert the data in batches\n",
    "        insert_query = '''\n",
    "        INSERT INTO FactStock (ticker, date, open, high, low, close)\n",
    "        VALUES %s\n",
    "        '''\n",
    "        \n",
    "        # Use execute_values for faster insertion\n",
    "        execute_values(cursor, insert_query, data_tuples)\n",
    "        conn.commit()\n",
    "        \n",
    "        # Count rows to verify\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM FactStock\")\n",
    "        row_count = cursor.fetchone()[0]\n",
    "        print(f\"Successfully imported {row_count} rows into FactStock table\")\n",
    "    else:\n",
    "        print(\"Error: Required columns not found in the CSV file\")\n",
    "        print(f\"Expected: ['ticker', 'date', 'open', 'high', 'low', 'close']\")\n",
    "        print(f\"Found: {df.columns.tolist()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Database Error: {e}\")\n",
    "    \n",
    "finally:\n",
    "    # Close the connection\n",
    "    if 'conn' in locals() and conn is not None:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"PostgreSQL connection closed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41c318fa",
   "metadata": {},
   "source": [
    "## Insert all"
   ]
  },
  {
   "cell_type": "code",
   "id": "de1fab12",
   "metadata": {},
   "source": [
    "def insert_stock_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Successfully connected to PostgreSQL database\")\n",
    "\n",
    "        # Create the FactStock table\n",
    "        create_table_query = '''\n",
    "        CREATE TABLE IF NOT EXISTS FactStock (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            ticker VARCHAR(10) NOT NULL,\n",
    "            date DATE NOT NULL,\n",
    "            open NUMERIC(10, 2),\n",
    "            high NUMERIC(10, 2),\n",
    "            low NUMERIC(10, 2),\n",
    "            close NUMERIC(10, 2)\n",
    "        );\n",
    "        '''\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        print(\"FactStock table created or already exists\")\n",
    "\n",
    "        # Select only the required columns\n",
    "        if set(['ticker', 'date', 'open', 'high', 'low', 'close']).issubset(set(df.columns)):\n",
    "            df_subset = df[['ticker', 'date', 'open', 'high', 'low', 'close']]\n",
    "            \n",
    "            # Convert date column to datetime if it's not already\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df_subset['date']):\n",
    "                df_subset['date'] = pd.to_datetime(df_subset['date'])\n",
    "                \n",
    "            # Convert dataframe to list of tuples for faster insertion\n",
    "            data_tuples = list(df_subset.itertuples(index=False, name=None))\n",
    "            \n",
    "            # Insert the data in batches\n",
    "            insert_query = '''\n",
    "            INSERT INTO FactStock (ticker, date, open, high, low, close)\n",
    "            VALUES %s\n",
    "            '''\n",
    "            \n",
    "            # Use execute_values for faster insertion\n",
    "            execute_values(cursor, insert_query, data_tuples)\n",
    "            conn.commit()\n",
    "            \n",
    "            # Count rows to verify\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM FactStock\")\n",
    "            row_count = cursor.fetchone()[0]\n",
    "            print(f\"Successfully imported {row_count} rows into FactStock table\")\n",
    "        else:\n",
    "            print(\"Error: Required columns not found in the CSV file\")\n",
    "            print(f\"Expected: ['ticker', 'date', 'open', 'high', 'low', 'close']\")\n",
    "            print(f\"Found: {df.columns.tolist()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Database Error: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        # Close the connection\n",
    "        if 'conn' in locals() and conn is not None:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"PostgreSQL connection closed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "88f5c7ba",
   "metadata": {},
   "source": [
    "for file in files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        # Check if the file is a CSV file\n",
    "        print(f\"Processing {file}...\")\n",
    "        file_path = os.path.join(kaggle_raw_dir, file)\n",
    "        print(file, file_path)\n",
    "        insert_stock_data(file_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "028e385b",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac6d7a",
   "metadata": {},
   "source": [
    "## Get stock data for month"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4f72788",
   "metadata": {},
   "source": [
    "def get_stock_data(symbol, interval, start_date, end_date, custom_name=None):\n",
    "    \"\"\"\n",
    "    Fetch stock data from Twelve Data API.\n",
    "    \n",
    "    Parameters:\n",
    "    symbol (str): Stock symbol.\n",
    "    interval (str): Time interval (e.g., '1min', '1day').\n",
    "    start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "    end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing stock data.\n",
    "    \"\"\"\n",
    "    ts = td.time_series(\n",
    "        symbol=symbol,\n",
    "        interval=interval,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        timezone=\"America/New_York\"\n",
    "    )\n",
    "\n",
    "    df = ts.as_pandas()\n",
    "\n",
    "    file_name = f\"data/raw/{symbol}_{interval}_{start_date}_{end_date}.csv\"\n",
    "    if custom_name:\n",
    "        file_name = f\"data/raw/{custom_name}.csv\"\n",
    "\n",
    "    df.to_csv(file_name, index=True)\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dfbd62f5",
   "metadata": {},
   "source": [
    "get_stock_data(\"BA\", \"1day\", \"2024-01-01\", \"2024-02-28\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cd244b0",
   "metadata": {},
   "source": [
    "# Fetch daily time series data for SPY from 2024-01-01 to 2024-12-31\n",
    "ts = td.time_series(\n",
    "    symbol=\"BA\",\n",
    "    interval=\"1day\",\n",
    "    start_date=\"2024-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    timezone=\"America/New_York\"\n",
    ")\n",
    "\n",
    "# Convert the time series data to a pandas DataFrame\n",
    "df = ts.as_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save the DataFrame to a CSV file\n",
    "df.to_csv(\"data/raw/BA_2024_daily.csv\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d07dbaf0",
   "metadata": {},
   "source": [
    "## Insert stock data into db"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def update_database(df, table_name=\"FactStock\"):\n",
    "    \"\"\"Update PostgreSQL database with new data\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Add datetime column if not present\n",
    "        if 'datetime' not in df.columns:\n",
    "            df['datetime'] = df.index\n",
    "        \n",
    "        # Prepare data for insertion\n",
    "        data_tuples = list(df.itertuples(index=False, name=None))\n",
    "        \n",
    "        # Insert the data\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {table_name} (ticker, date, open, high, low, close)\n",
    "        VALUES %s\n",
    "        ON CONFLICT (ticker, date) DO UPDATE\n",
    "        SET open = EXCLUDED.open,\n",
    "            high = EXCLUDED.high,\n",
    "            low = EXCLUDED.low,\n",
    "            close = EXCLUDED.close\n",
    "        \"\"\"\n",
    "        \n",
    "        execute_values(cursor, insert_query, data_tuples)\n",
    "        conn.commit()\n",
    "        print(f\"Successfully updated {len(data_tuples)} rows in {table_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Database Error: {e}\")\n",
    "        if 'conn' in locals():\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if 'conn' in locals():\n",
    "            cursor.close()\n",
    "            conn.close()"
   ],
   "id": "67915bc4a76c0951"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
